{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy Webscraping Tutorial\n",
    "\n",
    "#### Table of Contents\n",
    "\n",
    "1. <a href='##Introduction'>Webscraping with Scrapy</a> <br>\n",
    "2. <a href='##Scrapy_Shell'>Using the Scrapy shell</a> <br>\n",
    "> 2.a <a href='##xpath'>Intro to data extraction with xpath</a> <br>\n",
    "3. <a href='##start_project'>Initializing a Scrapy project</a> <br>\n",
    "> 3.a <a href='##file_structure'>File structure of the Scrapy framework</a> <br>\n",
    "4. <a href='##example'>Scrapy Example</a> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Introduction'></a>\n",
    "\n",
    "### Webscraping with Scrapy\n",
    "\n",
    "What is **Scrapy** and why is this an important Python package for us to learn? There are several reasons why our understanding of webscraping packages is incomplete with a detailed analysis of this important tool for data extraction.\n",
    "\n",
    "* **Webscraping Framework**: **Scrapy** is a webscraping framework for large-scale webscraping projects. Where **Beautifulsoup** and **Selenium** (which we previously covered) are good tools for individual/small scale webscrapings tasks, **Scrapy** is built for professional level deployment <br><br>\n",
    "* **Multiple Requests**: While packages such as **Selenium** induce actions in HTML in a sequential manner, **Scrapy** makes requests in parallel with error handling. This means that if one request fails, the entire process will continue to run and process the next request. <br><br>\n",
    "* **Cloud Deployment**: Webcrawlers created in **Scrapy** can be deployed in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#Scrapy_Shell'></a>\n",
    "\n",
    "### The Scrapy Shell\n",
    "\n",
    "Scrapy has a unique feature that allows the user to generate an **http** request to a website, and then parse the website using **XPath** in the console. \n",
    "\n",
    "**`scrapy shell www.reddit.com`** will return a **`response`** object\n",
    "\n",
    "**`response.url`** will return the url we just passed\n",
    "\n",
    "**`response.body`** will display all of the html code from the request\n",
    "\n",
    "**`response.css`** will return all of the tags passed to the **`css`** argument\n",
    "\n",
    "Run the following code to create a collection of the **h2** objects which we can loop through and extract the text:\n",
    "\n",
    "`>>items = response.css('h2')` <br>\n",
    "`>>len(items)`\n",
    "\n",
    "`>>for item in items:`\n",
    " > `print(item.extract())`\n",
    "\n",
    "The **response** object returns the following attributes:\n",
    "* **url** (string) – the URL of this response\n",
    "* **status** (integer) – the HTTP status of the response.\n",
    "* **headers** (dict) – the headers of this response. \n",
    "* **flags** (list) – is a list containing the initial values for the Response.flags attribute. \n",
    "* **request** (Request object) – the initial value of the Response.request attribute. \n",
    "* **xpath** (response object) - specific tag we want to grab from the HTML page\n",
    "\n",
    "https://doc.scrapy.org/en/latest/topics/request-response.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#xpath'></a>\n",
    "\n",
    "### Introduction to XPath\n",
    "\n",
    "XPath allows us to define specific tags that we want to grab in the HTML code. XPath allows us to define these tags hierarchaly, which means they are more specific. For example, going back to our www.rottentomatoes.com website - suppose we want to grab all of the tags for movies which are opening this week, how can we define this query?\n",
    "\n",
    "![title](xpath1.png)\n",
    "\n",
    "We can grab these objects with the following xpath code:\n",
    "\n",
    "**`>>response.xpath(\"//tr[@class='sidebarInTheaterOpening']\")`**\n",
    "\n",
    "returns a list of objects, but not the specific attributes that we need (href, text). In order to get these, we need to dig deeper into the object:\n",
    "\n",
    "**`>>response.xpath('//tr[@class=\"sidebarInTheaterOpening\"]/td[@class=\"middle_col\"]/a/text()')`**\n",
    "\n",
    "**`>>response.xpath('//tr[@class=\"sidebarInTheaterOpening\"]/td[@class=\"middle_col\"]/a/@href')`**\n",
    "\n",
    "In the above example, we start by telling our xpath search to find all instances of **tr** tags since we start with the **//** operator.\n",
    "\n",
    "Next we say to find the **@class** items which are equal to **sidebarInTheaterOpening**\n",
    "\n",
    "Once we have all of these classes, we go down one level and find the **td** tag, where the **@class** attribute is equal to **middle_col**. \n",
    "\n",
    "We proceed another level lower and find the **a** tags. Once we have the **a** tags, we extract the **text()** and **@href** attributes and return these.\n",
    "\n",
    "* **//tr** - search the entire tree, starting from the root for the tag which follows this and return all **tr** instances\n",
    "* **//tr/a** - for each **tr** instance, find the **a** tag immediately underneath that tag\n",
    "* **//tr[@class='sidebarInTheaterOpening']** - for each **tr** tag, select the ones which have **@class** equal to 'sidebarInTheaterOpening'. \n",
    "* **//tr[@class='sidebarInTheaterOpening']/td/** - find the **td** tag immediately following the previous class attributes\n",
    "* **//tr[@class='sidebarInTheaterOpening']/td/text()** - extract the text associated with the following query\n",
    "\n",
    "For a more thorough tutorial on XPath, take a look at the following link:\n",
    "\n",
    "https://blog.scrapinghub.com/2016/10/27/an-introduction-to-xpath-with-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='#start_project'></a>\n",
    "\n",
    "### Initialize a Scrapy Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Step One\n",
    "\n",
    "We can initiliaze our Scrapy project by navigating to the directory where we want to store our code and typing:\n",
    "\n",
    "**`>>scrapy startproject scrapy_tutorial`**\n",
    "\n",
    "which will generate a file structure with the following hierarchy:\n",
    "\n",
    "![title](scrapyFileStructure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Step Two\n",
    "\n",
    "Next we create the scraper file for parsing our webpage. This scraper will be named and called through the command line. The code for the file looks like the following:\n",
    "\n",
    "![title](spiderFile.png)\n",
    "\n",
    "We run the scraper using the command \n",
    "\n",
    "**`>>scrapy crawl CFEM`**\n",
    "\n",
    "where \n",
    "\n",
    "**`name = \"CFEM\"`** is the name we set in the **`CFEMScraper.py`** file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#file_structure'></a>\n",
    "Our file tree now looks like the following:\n",
    "\n",
    "![title](scrapyFileStructureAfter.png)\n",
    "\n",
    "**Pro Tip:** Make sure to be in the top **'scrapy_tutorial'** (right next to **scrapy.cfg**) folder in order to run the webscraper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='#example'></a>\n",
    "\n",
    "### Scrapy Tutorial\n",
    "\n",
    "Now that we've covered a few of the basics, let's look at a specific example and cover some code with hands on application. Much of the documentation can be found at the following website, along with relevant examples:\n",
    "\n",
    "https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    "\n",
    "Additionally take a look at my **Scrapy** example found in the **scrapy_tutorial** example at:\n",
    "\n",
    "https://github.com/zachescalante/Zach-Escalante-Code/tree/master/scrapy_tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
